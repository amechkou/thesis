\chapter{Related Work}
\label{chap:1}

Text of the second chapter

\section{LLM Enhancement Techniques}
\label{sec:1}
2.1.1 prompt engineering
2.1.2 Retrieval-augmented-generation
2.1.3 Finetuning

\section{State of LLMs for the}
\label{sec:2}




Part b:

Evaluating therapeutic chatbots requires a clear functional framework. Viewing them merely as underdeveloped "digital therapists" overlooks their specific mechanisms of action. A more productive approach, proposed by Grodniewicz and Hohol (2024), conceptualizes these tools as cognitive-affective artifacts. This perspective reframes chatbots as external aids designed to assist users through specific functions, rather than mimicking human therapy.

According to this model, chatbots operate primarily by:

    Simulating Interaction: Offering a non-judgmental conversational space for users to externalize thoughts and feelings.

    Supporting Cognition: Acting as cognitive tools to help users structure thoughts, reframe cognitions, track moods, or access information.

    Altering Affect: Functioning as affective tools capable of modifying a user's emotional state through empathy, encouragement, or distraction.

Understanding chatbots through this functional lens is particularly crucial in professional contexts where they are designed to support human workers. The "artifact" model clarifies how an AI tool can augment an operator's abilities. For instance, an AI assistant can serve as a powerful cognitive artifact by rapidly retrieving information or suggesting ways to structure a complex conversation. It can also support affective tasks by proposing empathetic phrasing for the operator to use. This perspective helps define the AI's role as a supportive instrument that enhances professional capabilities, which is a central principle for its application in mental healthcare services.
(source: Therapeutic Chatbots as Cognitive-Affective Artifacts)



While the "artifact" model provides a conceptual framework for how a chatbot should support affect, the work of Li et al. provides a technical mechanism for how it can be achieved. In "Knowledge Bridging for Empathetic Dialogue Generation," the authors address a core failure of standard models: their inability to perceive implicit emotions. They argue that human empathy relies heavily on external knowledge (e.g., commonsense, emotional lexicons) to understand a situation, noting that there is almost no word overlap between a speaker's problem and an empathetic listener's response. Their solution, KEMP (Knowledge-aware EMPathetic dialogue generation), "bridges" this gap by constructing an "emotional context graph" that enriches the dialogue with external knowledge from sources like ConceptNet. This "knowledge-bridging" is directly relevant to the AI-CARES project; for an AI to serve as a supportive tool for an operator, it must be able to "explicitly understand and express emotions." This paper provides a clear precedent for using RAG-like techniques not just for retrieving factual data (a cognitive task), but for retrieving affective and commonsense knowledge to build a more accurate emotional model of the user, thereby enhancing the AI's role as a supportive "affective artifact."
(source:Knowledge Bridging for Empathetic Dialogue Generation)

Building on these concepts, Ma et al. (2025) address the challenge of controlling the empathetic output. They note that standard training fails to "align the empathy levels" between a generated response and an ideal human response. Their work adopts the definition of empathy from Sharma et al. (2020), which quantifies it across three communication mechanisms: emotional reaction (affective empathy), interpretation, and exploration (cognitive empathy). The authors propose EmpRL, a framework that uses reinforcement learning (RL) to train a model. The key innovation is an "empathy reward function" that explicitly rewards the model for aligning its output with the target empathy levels for all three mechanisms. This approach is central to the goals of this thesis, as it provides a concrete, computational method for both generating and evaluating empathetic responses. The three-part framework (reaction, interpretation, exploration) serves as a pre-designed, trainable rubric, directly informing the "qualitative benchmark (role-playing)" task described in the work plan.
(source: Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation)

A significant challenge in applying LLMs to mental healthcare is the inherent need to manage long-form dialogues, as extended conversational context can often degrade model accuracy and increase inference times. The work of Guo et al. (2025) on the "SouLLMate" system directly addresses this problem by proposing specific techniques to enhance long-context reasoning. Their system, which integrates RAG and advanced prompt engineering , introduces methods like Key Indicator Summarization (KIS) to distill key information from historical dialogues and Stacked Multi-Model Reasoning (SMMR) to improve the reliability of reasoning over extended inputs. This approach is highly relevant as it provides a clear precedent for the "state-of-the-art techniques" this thesis aims to benchmark. Furthermore, the SouLLMate project outlines a novel evaluation methodology using professionally annotated data , aligning directly with this thesis's goal of developing a systematic framework for validating LLM performance in mental health support scenarios.
(source: SouLLMate: An Application Enhancing Diverse Mental Health Support
with Adaptive LLMs, Prompt Engineering, and RAG Techniques)

While Retrieval-Augmented Generation (RAG) is a common method for providing LLMs with domain-specific knowledge, fine-tuning offers the potential to internalize this knowledge directly into the model's parameters, potentially reducing reliance on long prompts and external databases. However, standard supervised fine-tuning (SFT) often struggles to match RAG's effectiveness. Kujanpää et al. (2025) propose Prompt Distillation (PD), a self-distillation technique, as a more efficient method for knowledge injection from unstructured documents. In PD, the model being trained learns from its own output distributions generated when provided with the new knowledge in its prompt (acting as the 'teacher'), aiming to replicate this behavior without needing the knowledge in the prompt later (acting as the 'student'). This self-distillation approach avoids style and capacity mismatches inherent when learning from a different, often larger, expert model and leverages the richer information in probability distributions over target tokens (logits) rather than just the single 'correct' token used in SFT. The authors demonstrate that PD significantly outperforms SFT, requires less training data, and can achieve performance comparable or even superior to RAG in closed-book settings. Furthermore, combining PD with RAG can enhance performance beyond standard RAG alone. This makes Prompt Distillation a highly relevant technique to consider and potentially include in the comparative analysis planned for this thesis, as it presents a potentially more effective fine-tuning strategy for embedding specialized mental health knowledge into LLMs.
(source: Efficient Knowledge Injection in LLMs via Self-Distillation)


Text.
